{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20b95437-b3b3-4fda-a9ac-31bfa5a71aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08b6a116-c8e4-4a73-9676-c68062fc1fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "x = data.data # Feature matrix\n",
    "y = data.target # Target variable (0 for benign, 1 for malignant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fcaa0e51-59c9-473e-ab09-e9277e1a7e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values :\n",
      " 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Convert to DataFrame for easier inspection\n",
    "df = pd.DataFrame(x, columns = data.feature_names)\n",
    "print(\"Missing values :\\n\", df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7de5a258-95b2-43b8-8276-a7aa87936d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c2083be-4191-4cc1-af18-4d03ed520384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the feature matrix\n",
    "x_scaled = scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8cf964a-f22e-412e-8e7e-9250a2d10990",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = pd.DataFrame(y, columns=['Target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d0c7ee0-d484-454c-b6a8-4e0d680869ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To ensure the dataset is ready for modeling, let’s start by loading and preprocessing it:\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7282cb4-c481-48fc-a11d-4935f0b6a697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split dataset\n",
    "data = load_breast_cancer()\n",
    "x = data.data\n",
    "y = data.target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c99d5ae9-3344-4544-bd89-02c319dbcfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00e6a046-d7f1-4657-9d4a-2a0312b7287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features for algorithms sensitive to scale\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c2c364b-9b9f-4d58-ac3a-bc61f1aa6391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96        43\n",
      "           1       0.97      0.99      0.98        71\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.97      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "model_lr = LogisticRegression()\n",
    "model_lr.fit(X_train, Y_train)\n",
    "y_pred_lr = model_lr.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(classification_report(Y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9833f75-510d-482f-be51-2546c37db986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        43\n",
      "           1       0.96      0.96      0.96        71\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.94      0.94      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier\n",
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data  # Feature matrix\n",
    "y = data.target  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Decision Tree Classifier\n",
    "model_dt = DecisionTreeClassifier(random_state=42)\n",
    "model_dt.fit(X_train, y_train)\n",
    "y_pred_dt = model_dt.predict(X_test)\n",
    "\n",
    "print(\"Decision Tree Classifier Results:\")\n",
    "print(classification_report(y_test, y_pred_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3cc6d407-fd92-4823-a7c0-522d1a8500eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.95        43\n",
      "           1       0.96      0.99      0.97        71\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.96      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "model_rf = RandomForestClassifier(random_state = 42)\n",
    "model_rf.fit(X_train, Y_train)\n",
    "y_pred_rf = model_rf.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Classifier Results:\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f31e52ab-c923-4a9d-8219-d54560ea0715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94        43\n",
      "           1       0.97      0.96      0.96        71\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.95      0.96      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine (SVM)\n",
    "model_svm = SVC(kernel='linear')  # Linear kernel is commonly effective for binary classification\n",
    "model_svm.fit(X_train, y_train)\n",
    "y_pred_svm = model_svm.predict(X_test)\n",
    "\n",
    "print(\"Support Vector Machine Results:\")\n",
    "print(classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94396679-393b-4f2a-a693-e59556b5c39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-Nearest Neighbors Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        43\n",
      "           1       0.96      0.96      0.96        71\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.94      0.94      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# k-Nearest Neighbors (k-NN)\n",
    "model_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "model_knn.fit(X_train, y_train)\n",
    "y_pred_knn = model_knn.predict(X_test)\n",
    "\n",
    "print(\"k-Nearest Neighbors Results:\")\n",
    "print(classification_report(y_test, y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc2c979-a57f-44de-b311-2885fc577f18",
   "metadata": {},
   "source": [
    "1. Logistic Regression\n",
    "How it Works: Logistic Regression is a linear model for binary classification that estimates the probability of an instance belonging to a particular class by applying a logistic (sigmoid) function to a linear combination of input features. The output is a probability between 0 and 1, typically using a threshold of 0.5 for classification.\n",
    "\n",
    "Suitability: Logistic Regression is effective for binary classification problems like this dataset, where the goal is to classify tumors as benign or malignant. It’s a straightforward, interpretable model and performs well as a baseline when the data can be linearly separable.\n",
    "\n",
    "2. Decision Tree Classifier\n",
    "How it Works: A Decision Tree Classifier recursively splits the data into subsets based on feature values, forming a tree structure. Each split is chosen to best separate classes, leading to branches and leaf nodes that represent class labels. It’s a non-parametric algorithm, meaning it doesn’t assume any particular data distribution.\n",
    "\n",
    "Suitability: Decision Trees can capture complex, non-linear relationships in data and work well for datasets with mixed feature importance, such as this one. However, they can be prone to overfitting, especially with deep trees, so tuning the depth or ensemble methods (like Random Forests) can improve performance.\n",
    "\n",
    "3. Random Forest Classifier\n",
    "How it Works: Random Forest is an ensemble method that builds multiple decision trees using random subsets of the data and features. Each tree’s output is aggregated (typically by majority vote) to make the final prediction. This approach reduces overfitting by averaging out the high variance of individual trees.\n",
    "\n",
    "Suitability: Random Forests are ideal for datasets like this one, which contain complex patterns. They are more robust than single decision trees, usually yielding higher accuracy by averaging multiple predictions. Additionally, they provide feature importance metrics, which can help identify the most influential factors in classification.\n",
    "\n",
    "Support Vector Machine (SVM)\n",
    "How it Works: SVM finds a hyperplane in the feature space that maximizes the margin between the two classes. It uses support vectors (the data points closest to the hyperplane) to define this boundary. SVM can also be configured with different kernels (e.g., linear, polynomial, radial basis function) to capture non-linear relationships.\n",
    "\n",
    "Suitability: SVM is highly effective for binary classification, especially on datasets where classes are well-separated, as in this case. With a linear kernel, SVM works well for high-dimensional data like the 30 features in this dataset, providing a robust boundary for the benign and malignant classes. However, it may be computationally expensive on larger datasets.\n",
    "\n",
    "5. k-Nearest Neighbors (k-NN)\n",
    "How it Works: k-NN is an instance-based algorithm that assigns a class label to a data point based on the majority label among its k nearest neighbors in the feature space. It relies on a distance metric (typically Euclidean distance) to identify the closest neighbors.\n",
    "\n",
    "Suitability: k-NN is suitable for this dataset due to its simplicity and effectiveness in binary classification, particularly for smaller datasets. However, it is sensitive to feature scales, so standardization is essential. It may be less efficient for high-dimensional or noisy data, but with proper scaling, it can be effective here.\n",
    "\n",
    "Each algorithm offers unique strengths: Logistic Regression provides simplicity, Decision Trees and Random Forests capture complex patterns, SVM effectively handles high-dimensional data, and k-NN offers straightforward, distance-based classification. Testing each one on this dataset allows us to evaluate which approach best suits the classification of benign and malignant tumors based on their features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "80016bff-c9ca-4548-86d0-44a7ee490655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0f7690e-9830-4d01-9c1e-a834f2b7bbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Support Vector Machine': SVC(kernel='linear', random_state=42),\n",
    "    'k-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b66a4702-49cd-4753-ada9-c36018c37f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store model performance\n",
    "performance = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5de80901-3409-45d6-96ca-01617d64f9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, predict, and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Evaluate performance\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    performance[model_name] = {'Accuracy': accuracy, 'F1 Score': f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "55fae71c-4bff-49a5-bb1a-39aaec907af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Accuracy: 0.9737, F1 Score: 0.9790\n",
      "Decision Tree - Accuracy: 0.9474, F1 Score: 0.9577\n",
      "Random Forest - Accuracy: 0.9649, F1 Score: 0.9722\n",
      "Support Vector Machine - Accuracy: 0.9561, F1 Score: 0.9645\n",
      "k-Nearest Neighbors - Accuracy: 0.9474, F1 Score: 0.9577\n"
     ]
    }
   ],
   "source": [
    "# Display performance results\n",
    "for model_name, metrics in performance.items():\n",
    "    print(f\"{model_name} - Accuracy: {metrics['Accuracy']:.4f}, F1 Score: {metrics['F1 Score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307725cd-bb63-464c-9be0-41abd9568849",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
